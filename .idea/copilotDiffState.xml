<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/sanex/functions/_handle_outliers.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/sanex/functions/_handle_outliers.py" />
              <option name="originalContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;&#10;def handle_outliers(df: DataFrameType, method: str = 'zscore', threshold: float = 3.0) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Handles outliers in the DataFrame by removing rows containing outliers.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    method (str): Method to identify outliers. Options are 'zscore' or 'iqr'. Default is 'zscore'.&#10;    threshold (float): Threshold for identifying outliers. For 'zscore', it's the number of standard deviations.&#10;                       For 'iqr', it's the multiplier for the interquartile range. Default is 3.0.&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with outlier rows removed.&#10;    &quot;&quot;&quot;&#10;    if isinstance(df, pd.DataFrame):&#10;        numeric_cols = df.select_dtypes(include='number').columns&#10;&#10;        if method == 'zscore':&#10;            # Calculate Z-scores for all numeric columns at once&#10;            z_scores = df[numeric_cols].apply(lambda x: (x - x.mean()) / x.std())&#10;            # A row is kept if all its z-scores are within the threshold&#10;            keep_rows = (z_scores.abs() &lt;= threshold).all(axis=1)&#10;            return df[keep_rows]&#10;&#10;        elif method == 'iqr':&#10;            Q1 = df[numeric_cols].quantile(0.25)&#10;            Q3 = df[numeric_cols].quantile(0.75)&#10;            IQR = Q3 - Q1&#10;            lower_bound = Q1 - (threshold * IQR)&#10;            upper_bound = Q3 + (threshold * IQR)&#10;&#10;            # A row is kept if its values in all numeric columns are within the bounds&#10;            keep_rows = ((df[numeric_cols] &gt;= lower_bound) &amp; (df[numeric_cols] &lt;= upper_bound)).all(axis=1)&#10;            return df[keep_rows]&#10;&#10;        else:&#10;            raise ValueError(&quot;Method must be either 'zscore' or 'iqr'.&quot;)&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        # Use Polars selectors to get numeric columns&#10;        numeric_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns&#10;&#10;        if not numeric_cols:&#10;            return df  # Return original if no numeric columns&#10;&#10;        conditions = []&#10;        if method == 'zscore':&#10;            for col_name in numeric_cols:&#10;                mean = df[col_name].mean()&#10;                std = df[col_name].std()&#10;                condition = ((df[col_name] - mean) / std).abs() &lt;= threshold&#10;                conditions.append(condition)&#10;&#10;        elif method == 'iqr':&#10;            for col_name in numeric_cols:&#10;                Q1 = df[col_name].quantile(0.25)&#10;                Q3 = df[col_name].quantile(0.75)&#10;                IQR = Q3 - Q1&#10;                lower_bound = Q1 - (threshold * IQR)&#10;                upper_bound = Q3 + (threshold * IQR)&#10;                condition = (df[col_name] &gt;= lower_bound) &amp; (df[col_name] &lt;= upper_bound)&#10;                conditions.append(condition)&#10;&#10;        else:&#10;            raise ValueError(&quot;Method must be either 'zscore' or 'iqr'.&quot;)&#10;&#10;        # Combine all conditions: a row is kept if it meets the condition for all numeric columns.&#10;        # .is_null() is included to keep rows with missing values in that column.&#10;        final_condition = pl.all_horizontal([(c | pl.col(c.meta.output_name()).is_null()) for c in conditions])&#10;        return df.filter(final_condition)&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
              <option name="updatedContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;&#10;def handle_outliers(df: DataFrameType, method: str = 'zscore', factor: float = 3.0, subset: list = None) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Handles outliers in the DataFrame by removing rows containing outliers.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    method (str): Method to identify outliers. Options are 'zscore' or 'iqr'. Default is 'zscore'.&#10;    factor (float): Threshold for identifying outliers. For 'zscore', it's the number of standard deviations.&#10;                   For 'iqr', it's the multiplier for the interquartile range. Default is 3.0.&#10;    subset (list): List of column names to consider for outlier handling. Default is None (all numeric columns).&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with outlier rows removed.&#10;    &quot;&quot;&quot;&#10;    if isinstance(df, pd.DataFrame):&#10;        # Determine which columns to process&#10;        if subset:&#10;            # Filter subset to only include numeric columns that exist in the DataFrame&#10;            numeric_cols = [col for col in subset if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]&#10;            if not numeric_cols:&#10;                # Return the original DataFrame if no valid numeric columns were found&#10;                return df&#10;        else:&#10;            numeric_cols = df.select_dtypes(include='number').columns&#10;&#10;        if method == 'zscore':&#10;            # Calculate Z-scores for all numeric columns at once&#10;            z_scores = df[numeric_cols].apply(lambda x: (x - x.mean()) / x.std())&#10;            # A row is kept if all its z-scores are within the threshold&#10;            keep_rows = (z_scores.abs() &lt;= factor).all(axis=1)&#10;            return df[keep_rows]&#10;&#10;        elif method == 'iqr':&#10;            Q1 = df[numeric_cols].quantile(0.25)&#10;            Q3 = df[numeric_cols].quantile(0.75)&#10;            IQR = Q3 - Q1&#10;            lower_bound = Q1 - (factor * IQR)&#10;            upper_bound = Q3 + (factor * IQR)&#10;&#10;            # A row is kept if its values in all numeric columns are within the bounds&#10;            keep_rows = ((df[numeric_cols] &gt;= lower_bound) &amp; (df[numeric_cols] &lt;= upper_bound)).all(axis=1)&#10;            return df[keep_rows]&#10;        &#10;        else:&#10;            raise ValueError(&quot;Method must be either 'zscore' or 'iqr'.&quot;)&#10;    &#10;    elif isinstance(df, pl.DataFrame):&#10;        # Use Polars selectors to get numeric columns&#10;        numeric_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns&#10;&#10;        if subset:&#10;            # Filter subset to only include numeric columns that exist in the DataFrame&#10;            numeric_cols = [col for col in subset if col in numeric_cols]&#10;&#10;        if not numeric_cols:&#10;            return df  # Return original if no valid numeric columns in subset&#10;&#10;        conditions = []&#10;        if method == 'zscore':&#10;            for col_name in numeric_cols:&#10;                mean = df[col_name].mean()&#10;                std = df[col_name].std()&#10;                condition = ((df[col_name] - mean) / std).abs() &lt;= factor&#10;                conditions.append(condition)&#10;&#10;        elif method == 'iqr':&#10;            for col_name in numeric_cols:&#10;                Q1 = df[col_name].quantile(0.25)&#10;                Q3 = df[col_name].quantile(0.75)&#10;                IQR = Q3 - Q1&#10;                lower_bound = Q1 - (factor * IQR)&#10;                upper_bound = Q3 + (factor * IQR)&#10;                condition = (df[col_name] &gt;= lower_bound) &amp; (df[col_name] &lt;= upper_bound)&#10;                conditions.append(condition)&#10;&#10;        else:&#10;            raise ValueError(&quot;Method must be either 'zscore' or 'iqr'.&quot;)&#10;&#10;        # Combine all conditions: a row is kept if it meets the condition for all numeric columns.&#10;        # .is_null() is included to keep rows with missing values in that column.&#10;        final_condition = pl.all_horizontal([(c | pl.col(c.meta.output_name()).is_null()) for c in conditions])&#10;        return df.filter(final_condition)&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/sanex/functions/_missing_data.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/sanex/functions/_missing_data.py" />
              <option name="originalContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, Literal, Optional, List&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;def drop_missing(df: DataFrameType, axis: str = 'rows',&#10;                how: Literal['any', 'all'] = 'any',&#10;                thresh: Optional[int] = None,&#10;                subset: Optional[List[str]] = None) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Drops rows or columns with missing values from the DataFrame.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    axis (str): The axis to drop from. 'rows' to drop rows, 'columns' to drop columns. Default is 'rows'.&#10;    how (str): 'any' to drop if any NA values are present, 'all' to drop if all values are NA. Default is 'any'.&#10;    thresh (int): Require that many non-NA values to avoid dropping. Default is None.&#10;    subset (list): List of column names to consider for dropping when axis is 'rows'. Default is None.&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with missing values dropped.&#10;    &quot;&quot;&quot;&#10;    # Validate the axis input&#10;    if axis not in ['rows', 'columns']:&#10;        raise ValueError(&quot;Axis must be either 'rows' or 'columns'.&quot;)&#10;&#10;    if isinstance(df, pd.DataFrame):&#10;        # Map 'rows' to 0 and 'columns' to 1 for pandas&#10;        pandas_axis = 0 if axis == 'rows' else 1&#10;        return df.dropna(axis=pandas_axis, how=how, thresh=thresh, subset=subset)&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        if axis == 'rows':&#10;            return df.drop_nulls(subset=subset)&#10;        elif axis == 'columns':&#10;            # Polars does not have a direct method to drop columns with nulls based on a condition like pandas.&#10;            # We need to identify columns with any nulls and then drop them.&#10;            if how == 'any':&#10;                cols_to_drop = [col for col in df.columns if df[col].is_null().any()]&#10;            elif how == 'all':&#10;                cols_to_drop = [col for col in df.columns if df[col].is_null().all()]&#10;            else:&#10;                raise ValueError(&quot;For polars, 'how' must be 'any' or 'all' when dropping columns.&quot;)&#10;&#10;            # The 'thresh' and 'subset' parameters are not directly applicable to dropping columns in Polars in the same way as pandas.&#10;            # You would typically select the columns you want to keep instead.&#10;            if thresh is not None:&#10;                print(&#10;                    &quot;Warning: The 'thresh' parameter is not supported for dropping columns in Polars and will be ignored.&quot;)&#10;            if subset is not None and axis == 'columns':&#10;                print(&quot;Warning: The 'subset' parameter is not applicable when dropping columns and will be ignored.&quot;)&#10;&#10;            return df.drop(cols_to_drop)&#10;&#10;    # Ensure we always return a DataFrame for all paths&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)&#10;&#10;def fill_missing(df: DataFrameType, value: Union[int, float, str] = 0, subset: Optional[List[str]] = None) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Fills missing values in the DataFrame with a specified value.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    value (Union[int, float, str]): The value to replace missing values with. Default is 0.&#10;    subset (list): List of column names to consider for filling. Default is None (all columns).&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with missing values filled.&#10;    &quot;&quot;&quot;&#10;    if isinstance(df, pd.DataFrame):&#10;        if subset is None:&#10;            return df.fillna(value=value)&#10;        else:&#10;            df_copy = df.copy()&#10;            for col in subset:&#10;                if col in df_copy.columns:&#10;                    df_copy[col] = df_copy[col].fillna(value)&#10;            return df_copy&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        if subset is None:&#10;            return df.fill_null(value)&#10;        else:&#10;            df_copy = df.clone()&#10;            for col in subset:&#10;                if col in df_copy.columns:&#10;                    df_copy = df_copy.with_column(&#10;                        pl.when(pl.col(col).is_null()).then(value).otherwise(pl.col(col)).alias(col)&#10;                    )&#10;            return df_copy&#10;&#10;    else:&#10;        raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)&#10;&#10;" />
              <option name="updatedContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, Optional, List&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;def drop_missing(df: DataFrameType, axis: str = 'rows',&#10;                how: str = 'any',&#10;                thresh: Optional[int] = None,&#10;                subset: Optional[List[str]] = None) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Drops rows or columns with missing values from the DataFrame.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    axis (str): The axis to drop from. 'rows' to drop rows, 'columns' to drop columns. Default is 'rows'.&#10;    how (str): 'any' to drop if any NA values are present, 'all' to drop if all values are NA. Default is 'any'.&#10;    thresh (int): Require that many non-NA values to avoid dropping. Default is None.&#10;    subset (list): List of column names to consider for dropping when axis is 'rows'. Default is None.&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with missing values dropped.&#10;    &quot;&quot;&quot;&#10;    # Validate the axis input&#10;    if axis not in ['rows', 'columns']:&#10;        raise ValueError(&quot;Axis must be either 'rows' or 'columns'.&quot;)&#10;        &#10;    # Validate the how input&#10;    if how not in ['any', 'all']:&#10;        raise ValueError(&quot;how must be either 'any' or 'all'.&quot;)&#10;&#10;    if isinstance(df, pd.DataFrame):&#10;        # Map 'rows' to 0 and 'columns' to 1 for pandas&#10;        pandas_axis = 0 if axis == 'rows' else 1&#10;        return df.dropna(axis=pandas_axis, how=how, thresh=thresh, subset=subset)&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        if axis == 'rows':&#10;            return df.drop_nulls(subset=subset)&#10;        elif axis == 'columns':&#10;            # Polars does not have a direct method to drop columns with nulls based on a condition like pandas.&#10;            # We need to identify columns with any nulls and then drop them.&#10;            if how == 'any':&#10;                cols_to_drop = [col for col in df.columns if df[col].is_null().any()]&#10;            elif how == 'all':&#10;                cols_to_drop = [col for col in df.columns if df[col].is_null().all()]&#10;            else:&#10;                raise ValueError(&quot;For polars, 'how' must be 'any' or 'all' when dropping columns.&quot;)&#10;&#10;            # The 'thresh' and 'subset' parameters are not directly applicable to dropping columns in Polars in the same way as pandas.&#10;            # You would typically select the columns you want to keep instead.&#10;            if thresh is not None:&#10;                print(&#10;                    &quot;Warning: The 'thresh' parameter is not supported for dropping columns in Polars and will be ignored.&quot;)&#10;            if subset is not None and axis == 'columns':&#10;                print(&quot;Warning: The 'subset' parameter is not applicable when dropping columns and will be ignored.&quot;)&#10;&#10;            return df.drop(cols_to_drop)&#10;&#10;    # Ensure we always return a DataFrame for all paths&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)&#10;&#10;def fill_missing(df: DataFrameType, value: Union[int, float, str] = 0, subset: Optional[List[str]] = None) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Fills missing values in the DataFrame with a specified value.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    value (Union[int, float, str]): The value to replace missing values with. Default is 0.&#10;    subset (list): List of column names to consider for filling. Default is None (all columns).&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with missing values filled.&#10;    &quot;&quot;&quot;&#10;    if isinstance(df, pd.DataFrame):&#10;        if subset is None:&#10;            return df.fillna(value=value)&#10;        else:&#10;            df_copy = df.copy()&#10;            for col in subset:&#10;                if col in df_copy.columns:&#10;                    df_copy[col] = df_copy[col].fillna(value)&#10;            return df_copy&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        if subset is None:&#10;            return df.fill_null(value)&#10;        else:&#10;            df_copy = df.clone()&#10;            for col in subset:&#10;                if col in df_copy.columns:&#10;                    df_copy = df_copy.with_column(&#10;                        pl.when(pl.col(col).is_null()).then(value).otherwise(pl.col(col)).alias(col)&#10;                    )&#10;            return df_copy&#10;&#10;    else:&#10;        raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/sanex/functions/_standardize_booleans.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/sanex/functions/_standardize_booleans.py" />
              <option name="originalContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, Dict, Any, Optional&#10;&#10;# Define constants for recognized boolean and missing values&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;TRUE_VALUES = {'true', '1', 't', 'yes', 'y', 'on'}&#10;FALSE_VALUES = {'false', '0', 'f', 'no', 'n', 'off'}&#10;MISSING_VALUES = {'', 'nan', 'null', 'none'}&#10;ALL_VALUES = TRUE_VALUES | FALSE_VALUES | MISSING_VALUES&#10;&#10;&#10;def standardize_booleans(df: DataFrameType) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Standardizes boolean-like columns in the DataFrame to actual boolean types.&#10;    Recognizes various representations of true, false, and missing values.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with standardized boolean columns.&#10;    &quot;&quot;&quot;&#10;    if isinstance(df, pd.DataFrame):&#10;        # Create a mapping dictionary for efficient conversion.&#10;        # Explicitly type hint with `Any` to handle mixed bool/NA values.&#10;        mapping: Dict[str, Optional[bool]] = {v: True for v in TRUE_VALUES}&#10;        mapping.update({v: False for v in FALSE_VALUES})&#10;        mapping.update({v: None for v in MISSING_VALUES})&#10;&#10;        # Work on a copy to avoid modifying the original DataFrame&#10;        df_copy = df.copy()&#10;&#10;        # Iterate only over object/string columns that could be booleans&#10;        for col in df_copy.select_dtypes(include=['object', 'string']).columns:&#10;            # Clean the series to check if all values are boolean-like&#10;            # .astype(str) handles potential mixed types or NaNs gracefully&#10;            cleaned_series = df_copy[col].astype(str).str.lower().str.strip()&#10;&#10;            # If all values are in our defined set, perform the conversion&#10;            if cleaned_series.isin(ALL_VALUES).all():&#10;                df_copy[col] = cleaned_series.map(mapping).astype('boolean')&#10;&#10;        return df_copy&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        # Iterate through columns to find potential boolean columns&#10;        for col in df.columns:&#10;            # Skip columns that are already boolean&#10;            if df[col].dtype == pl.Boolean:&#10;                continue&#10;&#10;            # For string columns, check if they can be standardized&#10;            if df[col].dtype == pl.Utf8:&#10;                # Clean the column values&#10;                lower_col = df[col].str.to_lowercase().str.strip_chars()&#10;&#10;                # If all values are boolean-like, convert the column&#10;                if lower_col.is_in(list(ALL_VALUES)).all():&#10;                    df = df.with_column(&#10;                        pl.when(lower_col.is_in(list(TRUE_VALUES))).then(True)&#10;                        .when(lower_col.is_in(list(FALSE_VALUES))).then(False)&#10;                        .otherwise(None).alias(col)&#10;                    )&#10;        return df&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
              <option name="updatedContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, Dict, Any, Optional, List&#10;&#10;# Define constants for recognized boolean and missing values&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;DEFAULT_TRUE_VALUES = {'true', '1', 't', 'yes', 'y', 'on'}&#10;DEFAULT_FALSE_VALUES = {'false', '0', 'f', 'no', 'n', 'off'}&#10;DEFAULT_MISSING_VALUES = {'', 'nan', 'null', 'none'}&#10;&#10;&#10;def standardize_booleans(&#10;    df: DataFrameType, &#10;    true_values: List[str] = None, &#10;    false_values: List[str] = None, &#10;    subset: List[str] = None&#10;) -&gt; DataFrameType:&#10;    &quot;&quot;&quot;&#10;    Standardizes boolean-like columns in the DataFrame to actual boolean types.&#10;    Recognizes various representations of true, false, and missing values.&#10;&#10;    Parameters:&#10;    df (DataFrameType): Input DataFrame.&#10;    true_values (List[str], optional): List of values to be considered as True. &#10;                                       Default is ['yes', 'y', 'true', 't', '1', 'on'].&#10;    false_values (List[str], optional): List of values to be considered as False. &#10;                                        Default is ['no', 'n', 'false', 'f', '0', 'off'].&#10;    subset (List[str], optional): List of column names to consider for boolean standardization. &#10;                                  Default is None (all columns).&#10;&#10;    Returns:&#10;    DataFrameType: DataFrame with standardized boolean columns.&#10;    &quot;&quot;&quot;&#10;    # Use default values if not provided&#10;    true_set = set(v.lower() for v in true_values) if true_values else DEFAULT_TRUE_VALUES&#10;    false_set = set(v.lower() for v in false_values) if false_values else DEFAULT_FALSE_VALUES&#10;    missing_set = DEFAULT_MISSING_VALUES&#10;    all_values = true_set | false_set | missing_set&#10;&#10;    if isinstance(df, pd.DataFrame):&#10;        # Create a mapping dictionary for efficient conversion&#10;        mapping: Dict[str, Optional[bool]] = {v: True for v in true_set}&#10;        mapping.update({v: False for v in false_set})&#10;        mapping.update({v: None for v in missing_set})&#10;&#10;        # Work on a copy to avoid modifying the original DataFrame&#10;        df_copy = df.copy()&#10;        &#10;        # Determine which columns to process&#10;        columns_to_process = subset if subset else df_copy.select_dtypes(include=['object', 'string']).columns&#10;&#10;        # Iterate only over specified columns or object/string columns&#10;        for col in columns_to_process:&#10;            if col not in df_copy.columns:&#10;                continue&#10;                &#10;            # Skip non-string columns if they're in the subset&#10;            if subset and df_copy[col].dtype not in ['object', 'string']:&#10;                continue&#10;                &#10;            # Clean the series to check if all values are boolean-like&#10;            cleaned_series = df_copy[col].astype(str).str.lower().str.strip()&#10;&#10;            # If all values are in our defined set, perform the conversion&#10;            if cleaned_series.isin(all_values).all():&#10;                df_copy[col] = cleaned_series.map(mapping).astype('boolean')&#10;&#10;        return df_copy&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        # Determine which columns to process&#10;        columns_to_process = subset if subset else df.columns&#10;        &#10;        # Iterate through selected columns&#10;        for col in columns_to_process:&#10;            if col not in df.columns:&#10;                continue&#10;                &#10;            # Skip columns that are already boolean&#10;            if df[col].dtype == pl.Boolean:&#10;                continue&#10;&#10;            # For string columns, check if they can be standardized&#10;            if df[col].dtype == pl.Utf8:&#10;                # Clean the column values&#10;                lower_col = df[col].str.to_lowercase().str.strip_chars()&#10;&#10;                # If all values are boolean-like, convert the column&#10;                if lower_col.is_in(list(all_values)).all():&#10;                    df = df.with_column(&#10;                        pl.when(lower_col.is_in(list(true_set))).then(True)&#10;                        .when(lower_col.is_in(list(false_set))).then(False)&#10;                        .otherwise(None).alias(col)&#10;                    )&#10;        return df&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>