<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/src/sanex/functions/__init__.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/sanex/functions/__init__.py" />
              <option name="originalContent" value="from ._clean_column_names import(&#10;snakecase, camelcase, pascalcase,&#10;kebabcase, titlecase, lowercase,&#10;screaming_snakecase, clean_column_names)&#10;from ._remove_duplicates import remove_duplicates&#10;from ._enforce_data_types import enforce_data_types&#10;from ._missing_data import fill_missing, drop_missing&#10;from ._whitespace import remove_whitespace&#10;from ._replace_text import replace_text&#10;from ._drop_single_value_columns import drop_single_value_columns&#10;from ._handle_outliers import handle_outliers, cap_outliers, remove_outliers&#10;from ._standardize_booleans import standardize_booleans&#10;from ._summarize_missing_data import missing_data_summary&#10;from ._remove_unwanted_rows_and_cols import remove_unwanted_rows_and_cols&#10;from ._extract_and_clean_numeric import extract_and_clean_numeric, clean_numeric&#10;from ._extract_email import extract_email&#10;from ._extract_with_regex import extract_with_regex&#10;from ._extract_phone_numbers import extract_phone_numbers&#10;from ._remove_punctuation import remove_punctuation&#10;from ._remove_special_characters import remove_special_characters, remove_emojis, remove_non_ascii, remove_non_alphanumeric, remove_non_numeric&#10;from ._remove_pii import remove_pii&#10;&#10;&#10;__all__ = [&#10;    &quot;clean_column_names&quot;,&#10;    &quot;snakecase&quot;,&#10;    &quot;camelcase&quot;,&#10;    &quot;pascalcase&quot;,&#10;    &quot;kebabcase&quot;,&#10;    &quot;titlecase&quot;,&#10;    &quot;lowercase&quot;,&#10;    &quot;screaming_snakecase&quot;,&#10;    &quot;remove_duplicates&quot;,&#10;    &quot;enforce_data_types&quot;,&#10;    &quot;fill_missing&quot;,&#10;    &quot;drop_missing&quot;,&#10;    &quot;remove_whitespace&quot;,&#10;    &quot;replace_text&quot;,&#10;    &quot;drop_single_value_columns&quot;,&#10;    &quot;handle_outliers&quot;,&#10;    &quot;standardize_booleans&quot;,&#10;    &quot;cap_outliers&quot;,&#10;    &quot;remove_outliers&quot;,&#10;    &quot;missing_data_summary&quot;,&#10;    &quot;remove_unwanted_rows_and_cols&quot;,&#10;    &quot;extract_and_clean_numeric&quot;,&#10;    &quot;clean_numeric&quot;,&#10;    &quot;extract_email&quot;,&#10;    &quot;extract_with_regex&quot;,&#10;    &quot;extract_phone_numbers&quot;,&#10;    &quot;remove_punctuation&quot;,&#10;    &quot;remove_special_characters&quot;,&#10;    &quot;remove_emojis&quot;,&#10;    &quot;remove_non_ascii&quot;,&#10;    &quot;remove_non_alphanumeric&quot;,&#10;    &quot;remove_non_numeric&quot;,&#10;    &quot;remove_pii&quot;&#10;]" />
              <option name="updatedContent" value="from ._clean_column_names import(&#10;snakecase, camelcase, pascalcase,&#10;kebabcase, titlecase, lowercase,&#10;screaming_snakecase, clean_column_names)&#10;from ._remove_duplicates import remove_duplicates&#10;from ._enforce_data_types import enforce_data_types&#10;from ._missing_data import fill_missing, drop_missing&#10;from ._whitespace import remove_whitespace&#10;from ._replace_text import replace_text&#10;from ._drop_single_value_columns import drop_single_value_columns&#10;from ._handle_outliers import handle_outliers, cap_outliers, remove_outliers&#10;from ._standardize_booleans import standardize_booleans&#10;from ._summarize_missing_data import missing_data_summary&#10;from ._remove_unwanted_rows_and_cols import remove_unwanted_rows_and_cols&#10;from ._extract_and_clean_numeric import extract_and_clean_numeric, clean_numeric&#10;from ._extract_email import extract_email&#10;from ._extract_with_regex import extract_with_regex&#10;from ._extract_phone_numbers import extract_phone_numbers&#10;from ._remove_punctuation import remove_punctuation&#10;from ._remove_special_characters import remove_special_characters, remove_emojis, remove_non_ascii, remove_non_alphanumeric, remove_non_numeric&#10;from ._remove_pii import remove_pii&#10;from ._remove_stopwords import remove_stopwords&#10;&#10;&#10;__all__ = [&#10;    &quot;clean_column_names&quot;,&#10;    &quot;snakecase&quot;,&#10;    &quot;camelcase&quot;,&#10;    &quot;pascalcase&quot;,&#10;    &quot;kebabcase&quot;,&#10;    &quot;titlecase&quot;,&#10;    &quot;lowercase&quot;,&#10;    &quot;screaming_snakecase&quot;,&#10;    &quot;remove_duplicates&quot;,&#10;    &quot;enforce_data_types&quot;,&#10;    &quot;fill_missing&quot;,&#10;    &quot;drop_missing&quot;,&#10;    &quot;remove_whitespace&quot;,&#10;    &quot;replace_text&quot;,&#10;    &quot;drop_single_value_columns&quot;,&#10;    &quot;handle_outliers&quot;,&#10;    &quot;standardize_booleans&quot;,&#10;    &quot;cap_outliers&quot;,&#10;    &quot;remove_outliers&quot;,&#10;    &quot;missing_data_summary&quot;,&#10;    &quot;remove_unwanted_rows_and_cols&quot;,&#10;    &quot;extract_and_clean_numeric&quot;,&#10;    &quot;clean_numeric&quot;,&#10;    &quot;extract_email&quot;,&#10;    &quot;extract_with_regex&quot;,&#10;    &quot;extract_phone_numbers&quot;,&#10;    &quot;remove_punctuation&quot;,&#10;    &quot;remove_special_characters&quot;,&#10;    &quot;remove_emojis&quot;,&#10;    &quot;remove_non_ascii&quot;,&#10;    &quot;remove_non_alphanumeric&quot;,&#10;    &quot;remove_non_numeric&quot;,&#10;    &quot;remove_pii&quot;,&#10;    &quot;remove_stopwords&quot;&#10;]" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/src/sanex/functions/_remove_stopwords.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/src/sanex/functions/_remove_stopwords.py" />
              <option name="originalContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, List&#10;from nltk.corpus import stopwords&#10;from nltk.tokenize import word_tokenize&#10;import nltk&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;# Lazy ensure NLTK resources are available&#10;_DEF_RESOURCES = [&#10;    (&quot;tokenizers/punkt&quot;, &quot;punkt&quot;),&#10;    (&quot;corpora/stopwords&quot;, &quot;stopwords&quot;),&#10;]&#10;&#10;def _ensure_nltk_resources():&#10;    for path, pkg in _DEF_RESOURCES:&#10;        try:&#10;            nltk.data.find(path)&#10;        except LookupError:  # pragma: no cover (network dependent)&#10;            try:&#10;                nltk.download(pkg, quiet=True)&#10;            except Exception:&#10;                # If download fails, continue; function will error later if truly missing&#10;                pass&#10;&#10;def _remove_stopwords_from_text(text: str, stop_words: set) -&gt; str:&#10;    if pd.isna(text):&#10;        return text&#10;    # Tokenize&#10;    try:&#10;        words = word_tokenize(text)&#10;    except LookupError:  # In case resources somehow missing mid-run&#10;        _ensure_nltk_resources()&#10;        words = word_tokenize(text)&#10;    filtered_words = [word for word in words if word.lower() not in stop_words]&#10;    return ' '.join(filtered_words)&#10;&#10;def remove_stopwords(df: DataFrameType, subset: List[str], language: str = 'english') -&gt; DataFrameType:&#10;    &quot;&quot;&quot;Removes stopwords from specified string columns.&#10;&#10;    Parameters:&#10;        df: Input pandas or polars DataFrame.&#10;        subset: Columns to process (silently ignores missing columns).&#10;        language: Stopword language (default 'english').&#10;&#10;    Returns:&#10;        DataFrame with stopwords removed in target columns.&#10;    &quot;&quot;&quot;&#10;    _ensure_nltk_resources()&#10;    stop_words = set(stopwords.words(language))&#10;&#10;    if isinstance(df, pd.DataFrame):&#10;        for col in subset:&#10;            if col in df.columns and df[col].dtype in ['object', 'string']:&#10;                df[col] = df[col].apply(lambda x: _remove_stopwords_from_text(x, stop_words))&#10;        return df&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        # Process columns one by one using Python for compatibility (avoids unavailable Expr.apply)&#10;        df_copy = df.clone()&#10;        for col in subset:&#10;            if col in df_copy.columns and df_copy[col].dtype == pl.String:&#10;                processed = [&#10;                    _remove_stopwords_from_text(val, stop_words) for val in df_copy[col].to_list()&#10;                ]&#10;                df_copy = df_copy.with_columns(pl.Series(name=col, values=processed))&#10;        return df_copy&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
              <option name="updatedContent" value="import pandas as pd&#10;import polars as pl&#10;from typing import Union, List&#10;from nltk.corpus import stopwords&#10;from nltk.tokenize import word_tokenize&#10;import nltk&#10;&#10;DataFrameType = Union[pd.DataFrame, pl.DataFrame]&#10;&#10;# Lazy ensure NLTK resources are available&#10;_DEF_RESOURCES = [&#10;    (&quot;tokenizers/punkt&quot;, &quot;punkt&quot;),&#10;    (&quot;corpora/stopwords&quot;, &quot;stopwords&quot;),&#10;]&#10;&#10;def _ensure_nltk_resources():&#10;    for path, pkg in _DEF_RESOURCES:&#10;        try:&#10;            nltk.data.find(path)&#10;        except LookupError:  # pragma: no cover (network dependent)&#10;            try:&#10;                nltk.download(pkg, quiet=True)&#10;            except Exception:&#10;                pass  # Ignore download failure; fallback logic will handle&#10;&#10;def _tokenize(text: str) -&gt; List[str]:&#10;    try:&#10;        return word_tokenize(text)&#10;    except LookupError:&#10;        _ensure_nltk_resources()&#10;        try:&#10;            return word_tokenize(text)&#10;        except Exception:&#10;            # Final fallback: simple whitespace split&#10;            return text.split()&#10;&#10;def _remove_stopwords_from_text(text: str, stop_words: set) -&gt; str:&#10;    if pd.isna(text):&#10;        return text&#10;    words = _tokenize(str(text))&#10;    filtered_words = [word for word in words if word.lower() not in stop_words]&#10;    return ' '.join(filtered_words)&#10;&#10;def remove_stopwords(df: DataFrameType, subset: List[str], language: str = 'english') -&gt; DataFrameType:&#10;    &quot;&quot;&quot;Removes stopwords from specified string columns.&#10;&#10;    Parameters:&#10;        df: Input pandas or polars DataFrame.&#10;        subset: Columns to process (silently ignores missing columns).&#10;        language: Stopword language (default 'english').&#10;&#10;    Returns:&#10;        DataFrame with stopwords removed in target columns.&#10;    &quot;&quot;&quot;&#10;    _ensure_nltk_resources()&#10;    stop_words = set(stopwords.words(language))&#10;&#10;    if isinstance(df, pd.DataFrame):&#10;        for col in subset:&#10;            if col in df.columns and df[col].dtype in ['object', 'string']:&#10;                df[col] = df[col].apply(lambda x: _remove_stopwords_from_text(x, stop_words))&#10;        return df&#10;&#10;    elif isinstance(df, pl.DataFrame):&#10;        df_copy = df.clone()&#10;        for col in subset:&#10;            if col in df_copy.columns and df_copy[col].dtype == pl.String:&#10;                processed = [&#10;                    _remove_stopwords_from_text(val, stop_words) for val in df_copy[col].to_list()&#10;                ]&#10;                df_copy = df_copy.with_columns(pl.Series(name=col, values=processed))&#10;        return df_copy&#10;&#10;    raise TypeError(&quot;Input must be a pandas or polars DataFrame.&quot;)" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>